{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "08_03_Pipeline_NN_Dataset_1_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL8byZbevLaB"
      },
      "source": [
        "# 1. Preprocessing"
      ],
      "id": "nL8byZbevLaB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoBSNuKg9OCF"
      },
      "source": [
        "## 1.1. Imports"
      ],
      "id": "DoBSNuKg9OCF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NknQ-71c9ODk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd6ef04-907c-4568-dcfd-0c0b1b4bd84c"
      },
      "source": [
        "# Imports\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "id": "NknQ-71c9ODk",
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wELLfvkamgwx",
        "outputId": "ab26e04b-308e-4f7d-cfa9-da0164639bfa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "wELLfvkamgwx",
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxydM1icv1lN"
      },
      "source": [
        "## 1.2. Preprocessing Functions"
      ],
      "id": "WxydM1icv1lN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql_YtFKRHJJ7"
      },
      "source": [
        "# Load data\r\n",
        "\r\n",
        "def get_data(nrows=500_000):\r\n",
        "    '''returns a DataFrame with nrows from downloaded Keggle csv in raw_data folder'''\r\n",
        "    dataset_1 = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/dataset_1.csv\", nrows=nrows) \r\n",
        "    df = dataset_1.copy()\r\n",
        "    return df\r\n"
      ],
      "id": "ql_YtFKRHJJ7",
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFT7yd1_HZ_q"
      },
      "source": [
        "# Clean data\r\n",
        "\r\n",
        "def clean_data(df):\r\n",
        "    '''returns cleaned DataFrame'''\r\n",
        "    \r\n",
        "    # dropping redundant columns\r\n",
        "    df = df[['Negative_Review', 'Positive_Review', 'Reviewer_Score']]\r\n",
        "\r\n",
        "    # Cleaning, merging and renaming negative and positive reviews\r\n",
        "    df[['Negative_Review']] = df[['Negative_Review']].replace(to_replace=\"No Negative\", value=\"\")\r\n",
        "    df[['Positive_Review']] = df[['Positive_Review']].replace(to_replace=\"No Positive\", value=\"\")\r\n",
        "    df[\"reviews\"] = df['Negative_Review'] + \" \" + df['Positive_Review']\r\n",
        "    df[\"review_score\"] = df['Reviewer_Score']\r\n",
        "    df = df.drop(columns=['Negative_Review', 'Positive_Review', 'Reviewer_Score'])\r\n",
        "\r\n",
        "    # Remove reviews with less than 6 words (or signs)\r\n",
        "    df['length'] = df['reviews'].apply(lambda x: len(word_tokenize(str(x))))\r\n",
        "    df.drop(df[df['length'] < 6].index, inplace=True)\r\n",
        "    df.drop(columns=['length'], inplace=True)\r\n",
        "    df.reset_index(drop=True, inplace=True)\r\n",
        "\r\n",
        "    return df"
      ],
      "id": "iFT7yd1_HZ_q",
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpCdSMovIZ6c"
      },
      "source": [
        "# Balance data\r\n",
        "\r\n",
        "def balance_data(df):\r\n",
        "  df_1 = df[df['review_score'] < 5][:10000]\r\n",
        "  df_4 = df[(df['review_score'] > 9) & (df['review_score'] < 10.1)][:10000]\r\n",
        "  df = pd.concat([df_1,df_4])\r\n",
        "\r\n",
        "  return df"
      ],
      "id": "FpCdSMovIZ6c",
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lja15pRcW2hS"
      },
      "source": [
        "# Clean for NLP\r\n",
        "\r\n",
        "def custom_stopwords():\r\n",
        "    \"\"\"create custom stopwords list excluding negative words\"\"\"\r\n",
        "    negative_words = ['no',\r\n",
        "    'nor',\r\n",
        "    'not',\r\n",
        "    \"don't\",\r\n",
        "    'should',\r\n",
        "    \"should've\",\r\n",
        "    'aren',\r\n",
        "    \"aren't\",\r\n",
        "    'couldn',\r\n",
        "    \"couldn't\",\r\n",
        "    'didn',\r\n",
        "    \"didn't\",\r\n",
        "    'doesn',\r\n",
        "    \"doesn't\",\r\n",
        "    'hadn',\r\n",
        "    \"hadn't\",\r\n",
        "    'hasn',\r\n",
        "    \"hasn't\",\r\n",
        "    'haven',\r\n",
        "    \"haven't\",\r\n",
        "    'isn',\r\n",
        "    \"isn't\",\r\n",
        "    \"wasn't\",\r\n",
        "    'weren',\r\n",
        "    \"weren't\",\r\n",
        "    'won',\r\n",
        "    \"won't\",\r\n",
        "    'wouldn',\r\n",
        "    \"wouldn't\"]\r\n",
        "\r\n",
        "    custom_stopwords = [x for x in stopwords.words('english') if x not in negative_words]\r\n",
        "\r\n",
        "    extra_stopwords = [\"hotel\",\"everything\",\"anything\",\"thing\"]  #customize extra stop_words\r\n",
        "\r\n",
        "    custom_stopwords.extend(extra_stopwords)\r\n",
        "\r\n",
        "    return custom_stopwords\r\n",
        "\r\n",
        "\r\n",
        "def clean_for_nlp(text):\r\n",
        "    \"\"\" preprocess review text data for nlp analysis \"\"\"\r\n",
        "    # Lower case\r\n",
        "    text = ''.join(text)\r\n",
        "    text = text.lower()\r\n",
        "    # Remove numbers\r\n",
        "    text = ''.join(word for word in text if not word.isdigit())\r\n",
        "    # Remove punctuation\r\n",
        "    for punctuation in string.punctuation:\r\n",
        "        text = text.replace(punctuation, '')\r\n",
        "    # Remove stopwords\r\n",
        "    text = word_tokenize(text)\r\n",
        "    stopwords = custom_stopwords()\r\n",
        "    text = [w for w in text if not w in stopwords]\r\n",
        "    # Lemmatizing\r\n",
        "    lemmatizer = WordNetLemmatizer()\r\n",
        "    text = [lemmatizer.lemmatize(word) for word in text]\r\n",
        "    text = ' '.join(word for word in text)\r\n",
        "\r\n",
        "    return(text)"
      ],
      "id": "Lja15pRcW2hS",
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCG8nf3wqba0"
      },
      "source": [
        "# preprocess df for NN\r\n",
        "\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "\r\n",
        "def tokenInit(train, max_words=5000):\r\n",
        "    tokenizer = Tokenizer(num_words=max_words)\r\n",
        "    tokenizer.fit_on_texts(train)\r\n",
        "\r\n",
        "    return tokenizer\r\n",
        "\r\n",
        "\r\n",
        "def padding(X):\r\n",
        "  tokenizer = tokenInit(X)\r\n",
        "  sequences = tokenizer.texts_to_sequences(X)\r\n",
        "  X_pad = pad_sequences(sequences, dtype='int32', padding='post')\r\n",
        "\r\n",
        "  return X_pad"
      ],
      "id": "rCG8nf3wqba0",
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVJnujPRvcaf"
      },
      "source": [
        "## 1.3. Preprocessing Data (applying preprocessing functions to df)"
      ],
      "id": "VVJnujPRvcaf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        },
        "id": "4u7vNhhrm7M_",
        "outputId": "ed739b29-7893-4388-8daa-603e570f4990"
      },
      "source": [
        "# get data\r\n",
        "\r\n",
        "df = get_data()\r\n",
        "df = clean_data(df)\r\n",
        "df = balance_data(df)\r\n",
        "df"
      ],
      "id": "4u7vNhhrm7M_",
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:3069: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[k1] = value[k2]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviews</th>\n",
              "      <th>review_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I am so angry that i made this post available...</td>\n",
              "      <td>2.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>My room was dirty and I was afraid to walk ba...</td>\n",
              "      <td>3.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Cleaner did not change our sheet and duvet ev...</td>\n",
              "      <td>4.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>The floor in my room was filfy dirty Very bas...</td>\n",
              "      <td>4.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Our room was an overrated disaster room 231 d...</td>\n",
              "      <td>3.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20095</th>\n",
              "      <td>Definitely above expectations experience base...</td>\n",
              "      <td>9.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20099</th>\n",
              "      <td>Would have liked tea coffee making in room wa...</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20101</th>\n",
              "      <td>Clean comfortable and excellent service Noth...</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20109</th>\n",
              "      <td>Normal rooms are a bit small but that is Pari...</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20110</th>\n",
              "      <td>We liked everything  Loved the uniqueness of ...</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 reviews  review_score\n",
              "0       I am so angry that i made this post available...           2.9\n",
              "3       My room was dirty and I was afraid to walk ba...           3.8\n",
              "6       Cleaner did not change our sheet and duvet ev...           4.6\n",
              "12      The floor in my room was filfy dirty Very bas...           4.6\n",
              "21      Our room was an overrated disaster room 231 d...           3.8\n",
              "...                                                  ...           ...\n",
              "20095   Definitely above expectations experience base...           9.6\n",
              "20099   Would have liked tea coffee making in room wa...          10.0\n",
              "20101    Clean comfortable and excellent service Noth...          10.0\n",
              "20109   Normal rooms are a bit small but that is Pari...          10.0\n",
              "20110   We liked everything  Loved the uniqueness of ...          10.0\n",
              "\n",
              "[20000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lYR9PG3L10W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46913225-b284-4ea2-921b-a8f6e0c5ff59"
      },
      "source": [
        "# Define X and y\r\n",
        "\r\n",
        "X = df[['reviews']]\r\n",
        "y = df[['review_score']]\r\n",
        "\r\n",
        "# check\r\n",
        "print(X.shape, y.shape)"
      ],
      "id": "9lYR9PG3L10W",
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000, 1) (20000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4LkXPsUwPH-"
      },
      "source": [
        "# 2. Model"
      ],
      "id": "Z4LkXPsUwPH-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_3UnBBw9OIj"
      },
      "source": [
        " ## 2.1. Hold out"
      ],
      "id": "D_3UnBBw9OIj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeZmiatu9OIk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cc9a9c7-6f14-418a-9056-e2f87b6977d7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Hold out \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "#check\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "id": "ZeZmiatu9OIk",
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((16000, 1), (4000, 1), (16000, 1), (4000, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLNOtDBS9OJe"
      },
      "source": [
        " ## 2.2. NN"
      ],
      "id": "rLNOtDBS9OJe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwLys6P-zIba"
      },
      "source": [
        "### 2.2.1. Model Architecture"
      ],
      "id": "YwLys6P-zIba"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsLzkc_I9OJe"
      },
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "\n",
        "def initialize_model():\n",
        "    ### Model architecture\n",
        "    model = models.Sequential()\n",
        " \n",
        "    ### Embedding Padded\n",
        "    model.add(layers.Embedding(input_dim=5000, output_dim=100, mask_zero=True))\n",
        "        \n",
        "    ### First convolution & max-pooling\n",
        "    model.add(layers.LSTM(units=100, activation='tanh', return_sequences=True))\n",
        "    model.add(layers.LSTM(units=100, activation='tanh', return_sequences=True))\n",
        "    model.add(layers.LSTM(units=50, activation='tanh'))\n",
        "    model.add(layers.Dropout(0.2))                     #change params\n",
        "    model.add(layers.Dense(40, activation='relu', kernel_regularizer=regularizers.L1(0.01)))    #Use regulazers\n",
        "    model.add(layers.Dropout(0.2))                     #change params\n",
        "    model.add(layers.Dense(20, activation='relu', kernel_regularizer=regularizers.L1(0.01)))    #Use regulazers\n",
        "    model.add(layers.Dropout(0.2))                     #change params\n",
        "    model.add(layers.Dense(10, activation='relu', kernel_regularizer=regularizers.L1(0.01)))    #Use regulazers\n",
        "    model.add(layers.Dropout(0.2))                     #change params \n",
        "\n",
        "    ### Last layer (let's say a classification with 10 output)\n",
        "    model.add(layers.Dense(1, activation='linear'))\n",
        "        \n",
        "    ### Model compilation\n",
        "    model.compile(loss='mse', \n",
        "                  optimizer='rmsprop',\n",
        "                  metrics=['mae'])     \n",
        "\n",
        "    return model"
      ],
      "id": "xsLzkc_I9OJe",
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62WrQDyuMt9C"
      },
      "source": [
        "### 2.2.2. Make Model Pickleable"
      ],
      "id": "62WrQDyuMt9C"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7jdOLECMq3j"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.python.keras.layers import deserialize, serialize\r\n",
        "from tensorflow.python.keras.saving import saving_utils\r\n",
        "\r\n",
        "\r\n",
        "def unpack(model, training_config, weights):\r\n",
        "    restored_model = deserialize(model)\r\n",
        "    if training_config is not None:\r\n",
        "        restored_model.compile(\r\n",
        "            **saving_utils.compile_args_from_training_config(\r\n",
        "                training_config\r\n",
        "            )\r\n",
        "        )\r\n",
        "    restored_model.set_weights(weights)\r\n",
        "    return restored_model\r\n",
        "\r\n",
        "# Hotfix function\r\n",
        "def make_keras_picklable():\r\n",
        "\r\n",
        "    def __reduce__(self):\r\n",
        "        model_metadata = saving_utils.model_metadata(self)\r\n",
        "        training_config = model_metadata.get(\"training_config\", None)\r\n",
        "        model = serialize(self)\r\n",
        "        weights = self.get_weights()\r\n",
        "        return (unpack, (model, training_config, weights))\r\n",
        "\r\n",
        "    cls = Model\r\n",
        "    cls.__reduce__ = __reduce__\r\n",
        "\r\n",
        "# Run the function\r\n",
        "make_keras_picklable()"
      ],
      "id": "i7jdOLECMq3j",
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGo6nH9tzRRp"
      },
      "source": [
        "### 2.2.3. Initialize Model"
      ],
      "id": "GGo6nH9tzRRp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r6i2ZdD9OJf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b033d052-7358-4545-b16d-f39bd935fedb"
      },
      "source": [
        "# initialize model (actually not necessary here, just to overview summary)\n",
        "\n",
        "model = initialize_model()\n",
        "model.summary()"
      ],
      "id": "-r6i2ZdD9OJf",
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (None, None, 100)         500000    \n",
            "_________________________________________________________________\n",
            "lstm_33 (LSTM)               (None, None, 100)         80400     \n",
            "_________________________________________________________________\n",
            "lstm_34 (LSTM)               (None, None, 100)         80400     \n",
            "_________________________________________________________________\n",
            "lstm_35 (LSTM)               (None, 50)                30200     \n",
            "_________________________________________________________________\n",
            "dropout_44 (Dropout)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 40)                2040      \n",
            "_________________________________________________________________\n",
            "dropout_45 (Dropout)         (None, 40)                0         \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 20)                820       \n",
            "_________________________________________________________________\n",
            "dropout_46 (Dropout)         (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 10)                210       \n",
            "_________________________________________________________________\n",
            "dropout_47 (Dropout)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 694,081\n",
            "Trainable params: 694,081\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwW8AbGu9OJl"
      },
      "source": [
        "# 3. Pipelining, Fitting and Exporting Model"
      ],
      "id": "lwW8AbGu9OJl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNvZ2knqf_AF"
      },
      "source": [
        "## 3.1. Built Wrapper for Keras Model (to save it into a .joblib format)"
      ],
      "id": "gNvZ2knqf_AF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG4TG6zB3-C8"
      },
      "source": [
        "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\r\n",
        "\r\n",
        "nn_model = KerasRegressor(build_fn = initialize_model)"
      ],
      "id": "TG4TG6zB3-C8",
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeE9d3CGgV2-"
      },
      "source": [
        "## 3.2. Custom Transformer for Pipeline"
      ],
      "id": "FeE9d3CGgV2-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tYL344939uv"
      },
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
        "\r\n",
        "class TextProcessor(BaseEstimator, TransformerMixin):\r\n",
        "  \"\"\" Custom Transformer for cleaning and preprocessing string into required format for NN model \"\"\"\r\n",
        "  \r\n",
        "  def __init__(self, max_words=5000):\r\n",
        "    self.tokenizer = Tokenizer(num_words=max_words)\r\n",
        "  \r\n",
        "  def fit(self, X, y=None):\r\n",
        "    # cleaning text\r\n",
        "    X = list(map(clean_for_nlp, X['reviews']))\r\n",
        "    self.tokenizer.fit_on_texts(X)\r\n",
        "    return self\r\n",
        "\r\n",
        "  def transform(self, X, y=None):\r\n",
        "    # cleaning text\r\n",
        "    X = list(map(clean_for_nlp, X['reviews']))\r\n",
        "    # tokenizing\r\n",
        "    sequences = self.tokenizer.texts_to_sequences(X)\r\n",
        "    # padding\r\n",
        "    X = pad_sequences(sequences, dtype='int32', padding='post')\r\n",
        "\r\n",
        "    return X\r\n"
      ],
      "id": "5tYL344939uv",
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pg8GE7ogjPW"
      },
      "source": [
        "## 3.3. Build Pipeline"
      ],
      "id": "1pg8GE7ogjPW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdtYkH5x39DP"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\r\n",
        "from sklearn.compose import ColumnTransformer\r\n",
        "\r\n",
        "def set_pipeline():\r\n",
        "  \"\"\"defines the pipeline\"\"\"\r\n",
        "  preproc_pipe = Pipeline([('text_preprocessor', TextProcessor())])\r\n",
        "\r\n",
        "  pipeline = Pipeline([('preproc_pipe', preproc_pipe), ('nn_model', nn_model)])\r\n",
        "\r\n",
        "  return pipeline"
      ],
      "id": "EdtYkH5x39DP",
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsSBA6jS_QXf"
      },
      "source": [
        "# Set pipeline and initialize model\r\n",
        "pipeline = set_pipeline()"
      ],
      "id": "KsSBA6jS_QXf",
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj2ZANT-gsVq"
      },
      "source": [
        "## 3.4. Fit Pipeline"
      ],
      "id": "Jj2ZANT-gsVq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Djx3d5KHSH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8d1d4d8-a104-4da7-bc28-2b93c62941c4"
      },
      "source": [
        "# Fitting\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping\r\n",
        "\r\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\r\n",
        "\r\n",
        "pipeline.fit(X_train, y_train,\r\n",
        "          nn_model__validation_split=0.2,\r\n",
        "          nn_model__batch_size=32,\r\n",
        "          nn_model__epochs=200,\r\n",
        "          nn_model__verbose=1,\r\n",
        "          nn_model__callbacks=[es])"
      ],
      "id": "0Djx3d5KHSH-",
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "400/400 [==============================] - 20s 34ms/step - loss: 22.0121 - mae: 3.4850 - val_loss: 4.6396 - val_mae: 1.1304\n",
            "Epoch 2/200\n",
            "400/400 [==============================] - 11s 28ms/step - loss: 7.9244 - mae: 1.8430 - val_loss: 3.5125 - val_mae: 0.8798\n",
            "Epoch 3/200\n",
            "400/400 [==============================] - 11s 28ms/step - loss: 6.0561 - mae: 1.5665 - val_loss: 3.6758 - val_mae: 1.0746\n",
            "Epoch 4/200\n",
            "400/400 [==============================] - 11s 27ms/step - loss: 5.2487 - mae: 1.4506 - val_loss: 3.5867 - val_mae: 1.1403\n",
            "Epoch 5/200\n",
            "400/400 [==============================] - 12s 29ms/step - loss: 4.4807 - mae: 1.3519 - val_loss: 2.8382 - val_mae: 0.8503\n",
            "Epoch 6/200\n",
            "400/400 [==============================] - 11s 28ms/step - loss: 4.2329 - mae: 1.3382 - val_loss: 2.5895 - val_mae: 0.7870\n",
            "Epoch 7/200\n",
            "400/400 [==============================] - 11s 28ms/step - loss: 3.8506 - mae: 1.2770 - val_loss: 2.5529 - val_mae: 0.7661\n",
            "Epoch 8/200\n",
            "400/400 [==============================] - 11s 28ms/step - loss: 3.4771 - mae: 1.2210 - val_loss: 2.6328 - val_mae: 0.9586\n",
            "Epoch 9/200\n",
            "400/400 [==============================] - 11s 28ms/step - loss: 3.3851 - mae: 1.2142 - val_loss: 2.5186 - val_mae: 0.7767\n",
            "Epoch 10/200\n",
            "400/400 [==============================] - 11s 28ms/step - loss: 3.1245 - mae: 1.1627 - val_loss: 2.8633 - val_mae: 0.7914\n",
            "Epoch 11/200\n",
            "400/400 [==============================] - 11s 27ms/step - loss: 3.0937 - mae: 1.1460 - val_loss: 2.5634 - val_mae: 0.9264\n",
            "Epoch 12/200\n",
            "400/400 [==============================] - 11s 27ms/step - loss: 2.9335 - mae: 1.1268 - val_loss: 2.9570 - val_mae: 1.1854\n",
            "Epoch 13/200\n",
            "400/400 [==============================] - 11s 28ms/step - loss: 2.8263 - mae: 1.1081 - val_loss: 2.4565 - val_mae: 0.8820\n",
            "Epoch 14/200\n",
            "400/400 [==============================] - 11s 28ms/step - loss: 2.6842 - mae: 1.0609 - val_loss: 2.5953 - val_mae: 0.8852\n",
            "Epoch 15/200\n",
            "400/400 [==============================] - 11s 28ms/step - loss: 2.7355 - mae: 1.0830 - val_loss: 2.4689 - val_mae: 0.7927\n",
            "Epoch 16/200\n",
            "400/400 [==============================] - 11s 28ms/step - loss: 2.6823 - mae: 1.0653 - val_loss: 2.8989 - val_mae: 1.1457\n",
            "Epoch 17/200\n",
            "400/400 [==============================] - 11s 28ms/step - loss: 2.6036 - mae: 1.0482 - val_loss: 2.7077 - val_mae: 0.9836\n",
            "Epoch 18/200\n",
            "400/400 [==============================] - 11s 28ms/step - loss: 2.3992 - mae: 0.9955 - val_loss: 2.6908 - val_mae: 0.9493\n",
            "Epoch 19/200\n",
            "400/400 [==============================] - 12s 29ms/step - loss: 2.2921 - mae: 0.9758 - val_loss: 2.5489 - val_mae: 0.8446\n",
            "Epoch 20/200\n",
            "400/400 [==============================] - 12s 29ms/step - loss: 2.3334 - mae: 0.9752 - val_loss: 2.6933 - val_mae: 0.9116\n",
            "Epoch 21/200\n",
            "400/400 [==============================] - 11s 28ms/step - loss: 2.3355 - mae: 0.9843 - val_loss: 2.6459 - val_mae: 0.9807\n",
            "Epoch 22/200\n",
            "400/400 [==============================] - 11s 28ms/step - loss: 2.2516 - mae: 0.9648 - val_loss: 2.6328 - val_mae: 0.9543\n",
            "Epoch 23/200\n",
            "400/400 [==============================] - 11s 28ms/step - loss: 2.1786 - mae: 0.9559 - val_loss: 2.5541 - val_mae: 0.7653\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00023: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:197: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('preproc_pipe',\n",
              "                 Pipeline(memory=None,\n",
              "                          steps=[('text_preprocessor',\n",
              "                                  TextProcessor(max_words=None))],\n",
              "                          verbose=False)),\n",
              "                ('nn_model',\n",
              "                 <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7fb5f0363790>)],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdiUqpcAz6lV"
      },
      "source": [
        "## 3.5. Evaluate Model"
      ],
      "id": "gdiUqpcAz6lV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_rkFhcPz6RB",
        "outputId": "b8f64864-2073-4186-8acb-7b012b7a8293"
      },
      "source": [
        "pipeline.score(X_test,y_test)"
      ],
      "id": "S_rkFhcPz6RB",
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "125/125 [==============================] - 1s 11ms/step - loss: 2.5166 - mae: 0.8812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEoenT2kgyZB"
      },
      "source": [
        "## 3.6. Export to .joblib Format"
      ],
      "id": "EEoenT2kgyZB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDdqAR1g37aj"
      },
      "source": [
        "import joblib\r\n",
        "from termcolor import colored\r\n",
        "\r\n",
        "def save_model(pipeline):\r\n",
        "  \"\"\"Save the model into a .joblib format\"\"\"\r\n",
        "  joblib.dump(pipeline, 'model.joblib')\r\n",
        "  print(colored(\"model.joblib saved locally\", \"green\"))"
      ],
      "id": "vDdqAR1g37aj",
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1TQbpLWbea_",
        "outputId": "20673df3-e945-4a09-bf04-e33d5b1df78f"
      },
      "source": [
        "save_model(pipeline)"
      ],
      "id": "a1TQbpLWbea_",
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32mmodel.joblib saved locally\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwhn_7d4NgUy"
      },
      "source": [
        "pipeline_test = joblib.load('model.joblib')"
      ],
      "id": "rwhn_7d4NgUy",
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as1i5BMz9M7A"
      },
      "source": [
        "## 3.7. Test"
      ],
      "id": "as1i5BMz9M7A"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UExSuUNCNqih"
      },
      "source": [
        "Z = pd.DataFrame({\"reviews\": [\"This hotel was very nice, I loved it. Breakfast was amazing!\"]})"
      ],
      "id": "UExSuUNCNqih",
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfDG6A9MNuhB",
        "outputId": "91499ddd-e533-479e-83fb-f9aa753e125c"
      },
      "source": [
        "result = pipeline_test.predict(Z)\r\n",
        "result"
      ],
      "id": "hfDG6A9MNuhB",
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(9.147982, dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    }
  ]
}