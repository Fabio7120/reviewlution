{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import joblib\n",
    "from termcolor import colored\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for getting and cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define nrows:\n",
    "\n",
    "N = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(nrows=N):\n",
    "    '''returns a DataFrame with nrows from downloaded Keggle csv in raw_data folder'''\n",
    "    dataset_1 = pd.read_csv(\"../raw_data/dataset_1.csv\", nrows=nrows)\n",
    "    df = dataset_1.copy()\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    '''returns cleaned DataFrame'''\n",
    "    \n",
    "    # dropping redundant columns\n",
    "    df_clean = df[['Negative_Review', 'Positive_Review', 'Reviewer_Score']]\n",
    "\n",
    "    # Cleaning, merging and renaming negative and positive reviews\n",
    "    df_clean.loc[:,'Negative_Review'] = df_clean.loc[:,'Negative_Review'].replace(to_replace=\"No Negative\", value=\"\")\n",
    "    df_clean.loc[:,'Positive_Review'] = df_clean.loc[:,'Positive_Review'].replace(to_replace=\"No Positive\", value=\"\")\n",
    "    df_clean.loc[:,\"reviews\"] = df_clean.loc[:,'Negative_Review'] + \" \" + df_clean.loc[:,'Positive_Review']\n",
    "    df_clean.loc[:,\"review_score\"] = df_clean.loc[:,'Reviewer_Score']\n",
    "    df_clean = df_clean.drop(columns=['Negative_Review', 'Positive_Review', 'Reviewer_Score'])\n",
    "\n",
    "    # Remove reviews with less than 10 words (or signs)\n",
    "    df_clean.loc[:,'length'] = df_clean['reviews'].apply(lambda x: len(word_tokenize(str(x))))\n",
    "    df_clean.drop(df_clean[df_clean['length'] < 6].index, inplace=True)\n",
    "    df_clean.drop(columns=['length'], inplace=True)\n",
    "    df_clean.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for custom stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_stopwords():\n",
    "    \"\"\"create custom stopwords list excluding negative words\"\"\"\n",
    "    negative_words = ['no',\n",
    "    'nor',\n",
    "    'not',\n",
    "    \"don't\",\n",
    "    'should',\n",
    "    \"should've\",\n",
    "    'aren',\n",
    "    \"aren't\",\n",
    "    'couldn',\n",
    "    \"couldn't\",\n",
    "    'didn',\n",
    "    \"didn't\",\n",
    "    'doesn',\n",
    "    \"doesn't\",\n",
    "    'hadn',\n",
    "    \"hadn't\",\n",
    "    'hasn',\n",
    "    \"hasn't\",\n",
    "    'haven',\n",
    "    \"haven't\",\n",
    "    'isn',\n",
    "    \"isn't\",\n",
    "    \"wasn't\",\n",
    "    'weren',\n",
    "    \"weren't\",\n",
    "    'won',\n",
    "    \"won't\",\n",
    "    'wouldn',\n",
    "    \"wouldn't\"]\n",
    "\n",
    "    custom_stopwords = [x for x in stopwords.words('english') if x not in negative_words]\n",
    "\n",
    "    return custom_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_for_ml(text):\n",
    "    \"\"\" preprocess review text data for nlp analysis \"\"\"\n",
    "    # Lower case\n",
    "    text = ''.join(text)\n",
    "    text = text.lower()\n",
    "    # Remove numbers\n",
    "    text = ''.join(word for word in text if not word.isdigit())\n",
    "    # Remove punctuation\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    # Remove stopwords\n",
    "    text = word_tokenize(text)\n",
    "    stopwords = custom_stopwords()\n",
    "    text = [w for w in text if not w in stopwords]\n",
    "    # Lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    text = ' '.join(word for word in text)\n",
    "\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_for_dl(text):\n",
    "    \"\"\" preprocess review text data for nlp analysis \"\"\"\n",
    "    # Lower case\n",
    "    text = ''.join(text)\n",
    "    text = text.lower()\n",
    "    # Remove numbers\n",
    "    text = ''.join(word for word in text if not word.isdigit())\n",
    "    # Remove punctuation\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    # Remove stopwords\n",
    "    text = word_tokenize(text)\n",
    "    stopwords = custom_stopwords()\n",
    "    text = [w for w in text if not w in stopwords]\n",
    "    # Lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding functions for DL word2vec transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –– Step #1 split the sentence into tokens\n",
    "def convert_sentences(X):\n",
    "    return [sentence.split(' ') for sentence in X]\n",
    "\n",
    "\n",
    "# –– Step #2\n",
    "def embed_sentence(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv:\n",
    "            embedded_sentence.append(word2vec.wv[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "        ## transforming list of vectors into one vector\n",
    "        \n",
    "        sum_vec = embedded_sentence.sum(axis = 0)\n",
    "        \n",
    "        ## put zeros when sum_vec has invalid shape\n",
    "        if sum_vec.shape != (200,):\n",
    "            sum_vec = np.zeros(200)\n",
    "            \n",
    "        embed.append(sum_vec)\n",
    "        \n",
    "    ## transform a list into a np-matrix\n",
    "    return np.vstack(embed)\n",
    "\n",
    "\n",
    "def embedding_and_padding(text):\n",
    "    word2vec = Word2Vec(sentences=X_train, size=200, min_count=1, window=5)\n",
    "    ## convert into tokens\n",
    "    tokens = convert_sentences(text)\n",
    "    ## convert tokens into vectors\n",
    "    vectors = embedding(word2vec, tokens)\n",
    "    # padding the vectors\n",
    "    vectors_padding = pad_sequences(vectors, dtype='float32', padding='post')\n",
    "    \n",
    "    return vectors_padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes for processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLtextProcessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Custom Transformer for nlp-preprocessed data for ml analyses \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vectorizer = CountVectorizer(dtype=np.int32)\n",
    "\n",
    "    def fit(self, X_train, y_train=None):\n",
    "        X_transformed = list(map(clean_for_ml, X_train['reviews']))\n",
    "        self.vectorizer.fit(X_transformed)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X_train, y_train=None):\n",
    "        X_transformed = list(map(clean_for_ml, X_train['reviews']))\n",
    "        X_vectorized = self.vectorizer.transform(X_transformed).toarray()\n",
    "        return pd.DataFrame(X_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLtextProcessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Custom Transformer for nlp-preprocessed data for dl analyses  \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.X_transformed = X_train['reviews'].apply(clean_for_dl)\n",
    "\n",
    "    def fit(self, X_train, y_train=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X_train, y_train=None):\n",
    "        X_transformed = self.X_transformed.apply(clean_for_dl)\n",
    "        X_transformed = X_transformed.apply(embedding_and_padding)\n",
    "        \n",
    "        return pd.DataFrame(X_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for ml and dl pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_ml_pipeline():\n",
    "    \"\"\"defines the pipeline for machine learning models\"\"\"\n",
    "    nlp_transformer = Pipeline([('text_preprocessor', MLtextProcessor())])\n",
    "\n",
    "    preproc_pipe = ColumnTransformer([\n",
    "    ('nlp_transformer', nlp_transformer, [\"reviews\"])], remainder=\"drop\")\n",
    "\n",
    "    #pipeline = Pipeline([('preproc', preproc_pipe), ('linear_model', LinearRegression())])\n",
    "    \n",
    "    return preproc_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_dl_pipeline():\n",
    "    \"\"\"defines the pipeline for deep learning models\"\"\"\n",
    "\n",
    "    nlp_transformer = Pipeline([('text_preprocessor', DLtextProcessor())])\n",
    "\n",
    "    preproc_pipe = ColumnTransformer([\n",
    "    ('nlp_transformer', nlp_transformer, [\"reviews\"])], remainder=\"drop\")\n",
    "\n",
    "    return preproc_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and clean data and assign X,y, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kili/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/core/indexing.py:1675: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "/home/kili/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/pandas/core/indexing.py:1596: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n"
     ]
    }
   ],
   "source": [
    "df = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"review_score\"]\n",
    "X = df.drop(\"review_score\", axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciate preprocessing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_pipe_ml = set_ml_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_pipe_dl = set_dl_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
