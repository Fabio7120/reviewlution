{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../raw_data/clean_dataset_1.csv').drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df drop null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove comments les than x words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counter(X):\n",
    "    return len(X.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df['reviews'].apply(word_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['word_count']>9].reset_index().drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0    83043\n",
       "9.6     54462\n",
       "9.2     45878\n",
       "8.8     36744\n",
       "8.3     32612\n",
       "7.5     26691\n",
       "7.9     25785\n",
       "7.1     20052\n",
       "6.7     15658\n",
       "6.3     12556\n",
       "5.8     10281\n",
       "5.4      8058\n",
       "5.0      7114\n",
       "4.6      5343\n",
       "4.2      4349\n",
       "3.8      3468\n",
       "3.3      2296\n",
       "2.5      1660\n",
       "2.9      1320\n",
       "9.0       492\n",
       "9.5       491\n",
       "8.0       378\n",
       "8.5       378\n",
       "7.0       300\n",
       "6.5       298\n",
       "6.0       204\n",
       "5.5       196\n",
       "4.5        93\n",
       "4.0        72\n",
       "3.5        55\n",
       "9.4        46\n",
       "8.1        30\n",
       "3.0        30\n",
       "6.9        24\n",
       "5.6        12\n",
       "4.4        10\n",
       "3.1         7\n",
       "Name: reviewer_score, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['reviewer_score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df['reviews'][:60000]\n",
    "X_test = df['reviews'][60000:100000]\n",
    "\n",
    "y_train = df['reviewer_score'][:60000]\n",
    "y_test = df['reviewer_score'][60000:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2955"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(s) for s in X_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorizing und embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –– Step #1\n",
    "def convert_sentences(X):\n",
    "    return [sentence.split(' ') for sentence in X]\n",
    "\n",
    "X_train_words = convert_sentences(X_train)\n",
    "X_test_words = convert_sentences(X_test)\n",
    "\n",
    "# –– Step #2\n",
    "from gensim.models import Word2Vec\n",
    "word2vec = Word2Vec(sentences=X_train, size=200, min_count=1, window=5)\n",
    "\n",
    "# –– Step #3\n",
    "def embed_sentence(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv:\n",
    "            embedded_sentence.append(word2vec.wv[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# -- step 4\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n",
    "\n",
    "X_train_embed = embedding(word2vec, X_train_words)\n",
    "X_test_embed = embedding(word2vec, X_test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_embed, dtype='float32', padding='post')\n",
    "X_test_pad = pad_sequences(X_test_embed, dtype='float32', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68, 200)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, metrics\n",
    "\n",
    "\n",
    "def init_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(layers.LSTM(20, return_sequences=True, activation='tanh'))\n",
    "\n",
    "\n",
    "    model.add(layers.Dense(40, activation='relu'))\n",
    "    \n",
    "    \n",
    "    model.add(layers.Dense(20, activation='relu'))\n",
    "\n",
    "    ## output layer\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['mae', metrics.RootMeanSquaredError()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, metrics\n",
    "\n",
    "def init_model_2():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.LSTM(20, return_sequences=True, activation='tanh'))\n",
    "    \n",
    "    ## regularisation layer\n",
    "    model.add(layers.LSTM(10, activation='tanh'))\n",
    "    \n",
    "    ## regularisation layer\n",
    "    model.add(layers.Dense(5, activation='relu'))\n",
    "    \n",
    "    ## regularisation layer\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.compile(loss='mse', \n",
    "                  optimizer='rmsprop', \n",
    "                  metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model2 = init_model_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1313/1313 [==============================] - 94s 69ms/step - loss: 67.4695 - mae: 8.0456 - val_loss: 48.7039 - val_mae: 6.7243\n",
      "Epoch 2/100\n",
      "1313/1313 [==============================] - 86s 66ms/step - loss: 48.1052 - mae: 6.7343 - val_loss: 32.8030 - val_mae: 5.4164\n",
      "Epoch 3/100\n",
      "1313/1313 [==============================] - 85s 65ms/step - loss: 32.0916 - mae: 5.4216 - val_loss: 20.3460 - val_mae: 4.1604\n",
      "Epoch 4/100\n",
      "1313/1313 [==============================] - 85s 65ms/step - loss: 19.6578 - mae: 4.1488 - val_loss: 11.3291 - val_mae: 3.0306\n",
      "Epoch 5/100\n",
      "1313/1313 [==============================] - 88s 67ms/step - loss: 10.5834 - mae: 2.9679 - val_loss: 5.7425 - val_mae: 2.1226\n",
      "Epoch 6/100\n",
      "1313/1313 [==============================] - 82s 62ms/step - loss: 5.0524 - mae: 1.9995 - val_loss: 3.5498 - val_mae: 1.5857\n",
      "Epoch 7/100\n",
      "1313/1313 [==============================] - 79s 60ms/step - loss: 2.8094 - mae: 1.4073 - val_loss: 3.5948 - val_mae: 1.5028\n",
      "Epoch 8/100\n",
      "1313/1313 [==============================] - 79s 60ms/step - loss: 2.6485 - mae: 1.3146 - val_loss: 3.5853 - val_mae: 1.5027\n",
      "Epoch 9/100\n",
      "1313/1313 [==============================] - 81s 61ms/step - loss: 2.6415 - mae: 1.3150 - val_loss: 3.5960 - val_mae: 1.5028\n",
      "Epoch 10/100\n",
      "1313/1313 [==============================] - 89s 68ms/step - loss: 2.7025 - mae: 1.3323 - val_loss: 3.6043 - val_mae: 1.5029\n",
      "Epoch 11/100\n",
      "1313/1313 [==============================] - 87s 67ms/step - loss: 2.6870 - mae: 1.3234 - val_loss: 3.6014 - val_mae: 1.5029\n"
     ]
    }
   ],
   "source": [
    "# X_train_pad_short = X_train_pad[:500] # These two lines are just to accelerate the cell run\n",
    "# y_train_short = y_train[:500]\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model2.fit(X_train_pad, y_train, \n",
    "          batch_size = 32,\n",
    "          epochs=100,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.149195671081543, 1.5070407390594482]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model2.evaluate(X_test_pad, y_test, verbose=0)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"While am generally happy with their service, there is a push to take a considerable discount for a reservation that does not allow cancellations. The standard undiscounted price is about the same as booking directly with the option of cancellations. Using Booking.com is merely a convenience. The discount for forgoing cancellation needs to be comared with the cost of cancellation insurance. Even if one can't use the reservation because of government restrictions the hotels sock it to the customer for far more than their out of pocket costs since at worst they don't have to service the rooms and at best can re-rent them. Bottom line: Don't be taken in by Booking.com's apparently cheap nonrefundable offers.\"\n",
    "sentence2 = \"Great vacation until we tried to travel home. We tried calling and waited more than 2 hours for a callback and then they were unable/unwilling to help us. Stranded for 48 hours because of this company with no help rebooking flights. We are out for hotel, food, and time off work because I was hung up on repeatedly by their customer service department. Once I was finally home, they told me there's nothing they can do for me that they were really sorry all this happened. They were unwilling to make it right, but told me that I could have requested a refund for my flight home if I would have been able to reach them at the time.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [sentence1, sentence2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f556e88d820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.786937 ],\n",
       "       [7.7911944]], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## convert into tokens\n",
    "tokens = convert_sentences(lst)\n",
    "\n",
    "## convert tokens into vectors\n",
    "vectors = embedding(word2vec, tokens)\n",
    "\n",
    "# padding the vectors\n",
    "vectors_padding = pad_sequences(vectors, dtype='float32', padding='post')\n",
    "\n",
    "## predict\n",
    "prediction = model2.predict(vectors_padding)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_dataset1 = df['reviews'][100000:100100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_dataset1_rating = df['reviewer_score'][100000:100100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f556e88d820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "## convert into tokens\n",
    "tokens = convert_sentences(X_new_dataset1)\n",
    "\n",
    "## convert tokens into vectors\n",
    "vectors = embedding(word2vec, tokens)\n",
    "\n",
    "# padding the vectors\n",
    "vectors_padding = pad_sequences(vectors, dtype='float32', padding='post')\n",
    "\n",
    "## predict\n",
    "prediction = model2.predict(vectors_padding)\n",
    "\n",
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = prediction.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_comp = {'prediction': pred, 'real_score':X_new_dataset1_rating}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>real_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>[7.791843414306641]</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100005</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100006</th>\n",
       "      <td>[7.7932353019714355]</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100007</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100008</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100009</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100010</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100011</th>\n",
       "      <td>[7.749200344085693]</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100012</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100013</th>\n",
       "      <td>[7.780685901641846]</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100014</th>\n",
       "      <td>[7.755735397338867]</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100015</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100016</th>\n",
       "      <td>[7.65774393081665]</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100017</th>\n",
       "      <td>[7.7932353019714355]</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100018</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100019</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100020</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100021</th>\n",
       "      <td>[7.7932353019714355]</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100022</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100023</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100024</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100025</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100026</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100027</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100028</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100029</th>\n",
       "      <td>[7.7932353019714355]</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100030</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100031</th>\n",
       "      <td>[7.7932353019714355]</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100032</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100033</th>\n",
       "      <td>[7.791843414306641]</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100034</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100035</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100036</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100037</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100038</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100039</th>\n",
       "      <td>[7.7932353019714355]</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100040</th>\n",
       "      <td>[7.742398262023926]</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100041</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100042</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100043</th>\n",
       "      <td>[7.792099952697754]</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100044</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100045</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100046</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100047</th>\n",
       "      <td>[7.7932353019714355]</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100048</th>\n",
       "      <td>[7.7932353019714355]</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100049</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100050</th>\n",
       "      <td>[7.755735397338867]</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100051</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100052</th>\n",
       "      <td>[7.749200344085693]</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100053</th>\n",
       "      <td>[7.779017448425293]</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100054</th>\n",
       "      <td>[7.791843414306641]</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100055</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100056</th>\n",
       "      <td>[7.755735397338867]</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100057</th>\n",
       "      <td>[7.7932353019714355]</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100058</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100059</th>\n",
       "      <td>[7.79453182220459]</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  prediction  real_score\n",
       "100000   [7.791843414306641]         6.3\n",
       "100001    [7.79453182220459]         6.3\n",
       "100002    [7.79453182220459]         9.2\n",
       "100003    [7.79453182220459]         9.6\n",
       "100004    [7.79453182220459]         7.1\n",
       "100005    [7.79453182220459]         6.7\n",
       "100006  [7.7932353019714355]         8.3\n",
       "100007    [7.79453182220459]         8.8\n",
       "100008    [7.79453182220459]         8.3\n",
       "100009    [7.79453182220459]        10.0\n",
       "100010    [7.79453182220459]         9.6\n",
       "100011   [7.749200344085693]        10.0\n",
       "100012    [7.79453182220459]         7.5\n",
       "100013   [7.780685901641846]         6.3\n",
       "100014   [7.755735397338867]        10.0\n",
       "100015    [7.79453182220459]         7.5\n",
       "100016    [7.65774393081665]        10.0\n",
       "100017  [7.7932353019714355]         9.6\n",
       "100018    [7.79453182220459]        10.0\n",
       "100019    [7.79453182220459]         7.5\n",
       "100020    [7.79453182220459]         7.9\n",
       "100021  [7.7932353019714355]         9.6\n",
       "100022    [7.79453182220459]         9.6\n",
       "100023    [7.79453182220459]         8.8\n",
       "100024    [7.79453182220459]         6.7\n",
       "100025    [7.79453182220459]         9.2\n",
       "100026    [7.79453182220459]         6.7\n",
       "100027    [7.79453182220459]         7.1\n",
       "100028    [7.79453182220459]         7.5\n",
       "100029  [7.7932353019714355]         8.3\n",
       "100030    [7.79453182220459]         8.8\n",
       "100031  [7.7932353019714355]        10.0\n",
       "100032    [7.79453182220459]         4.6\n",
       "100033   [7.791843414306641]         5.4\n",
       "100034    [7.79453182220459]         8.8\n",
       "100035    [7.79453182220459]         5.0\n",
       "100036    [7.79453182220459]        10.0\n",
       "100037    [7.79453182220459]         7.1\n",
       "100038    [7.79453182220459]        10.0\n",
       "100039  [7.7932353019714355]         9.2\n",
       "100040   [7.742398262023926]         9.2\n",
       "100041    [7.79453182220459]         8.3\n",
       "100042    [7.79453182220459]        10.0\n",
       "100043   [7.792099952697754]         5.0\n",
       "100044    [7.79453182220459]         8.3\n",
       "100045    [7.79453182220459]         8.3\n",
       "100046    [7.79453182220459]         8.3\n",
       "100047  [7.7932353019714355]         4.6\n",
       "100048  [7.7932353019714355]         6.3\n",
       "100049    [7.79453182220459]         7.1\n",
       "100050   [7.755735397338867]         9.6\n",
       "100051    [7.79453182220459]        10.0\n",
       "100052   [7.749200344085693]         9.2\n",
       "100053   [7.779017448425293]         8.3\n",
       "100054   [7.791843414306641]         7.9\n",
       "100055    [7.79453182220459]         7.5\n",
       "100056   [7.755735397338867]         8.3\n",
       "100057  [7.7932353019714355]         8.8\n",
       "100058    [7.79453182220459]         8.3\n",
       "100059    [7.79453182220459]         4.6"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comp = pd.DataFrame(dct_comp)\n",
    "df_comp.head(60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
